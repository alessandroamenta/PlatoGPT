# PlatoGPT

## About the Project
This project is all about learning and having a bit of fun. I'm diving deeper into the world of ML, and a friend recently gifted me "The Republic" by Plato. The book has been sitting on my desk, staring at me every day. So, I decided, why not combine the two interests? Thus, this text generation model was born. It's trained on "The Republic" and built using the Transformer architecture. While I haven't read the book yet, the model generates text in the style of Plato. :)

## Architecture
The core of the project is built using the Transformer model, which consists of the following components:
- Self-Attention Heads
- Multi-Head Attention Layer
- Feed Forward Neural Network
- Transformer Block

## Training Data
The model is trained on the book "The Republic" by Plato. The text is tokenized and divided into training and validation datasets for efficient learning and evaluation.

## Usage
Once the model is trained, it can generate text that mimics the style and context of "The Republic".

Thanks for checking out this project! Happy learning and have fun experimenting!

